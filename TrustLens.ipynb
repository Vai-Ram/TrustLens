{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vai-Ram/Convolve-4.0-MAS---TrustLens/blob/main/TrustLens_Convolve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm flash_attn einops"
      ],
      "metadata": {
        "id": "xjzwEuBgt59A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Core AI & Database Libraries\n",
        "!pip install -q \\\n",
        "    qdrant-client \\\n",
        "    sentence-transformers \\\n",
        "    deepface \\\n",
        "    albumentations \\\n",
        "    accelerate \\\n",
        "    opencv-python-headless\n",
        "\n",
        "# 2. Install LlamaIndex (For Metadata Structuring)\n",
        "!pip install -q \\\n",
        "    llama-index-core \\\n",
        "    llama-index-llms-openai-like \\\n",
        "    llama-index-embeddings-huggingface\n",
        "\n",
        "\n",
        "print(\"‚úÖ All Dependencies Installed.\")"
      ],
      "metadata": {
        "id": "33-CmNlPyPwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.49.0 accelerate"
      ],
      "metadata": {
        "id": "NLNzt4a6yEFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# 1. FIX: Install missing dependency 'zstd'\n",
        "print(\"üîß Installing dependencies...\")\n",
        "!sudo apt-get install -y zstd\n",
        "\n",
        "# 2. Install Ollama\n",
        "print(\"‚è≥ Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 3. Start the Server in the Background\n",
        "print(\"üöÄ Starting Ollama Server...\")\n",
        "process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# 4. Wait for it to spin up\n",
        "print(\"‚è≥ Waiting 10 seconds for server to initialize...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# 5. Pull the Model\n",
        "print(\"‚¨áÔ∏è Downloading Model (gemma3:4b)...\")\n",
        "!ollama pull gemma3:4b\n",
        "\n",
        "print(\"‚úÖ Setup Complete! Now you can run the TextAgent code.\")"
      ],
      "metadata": {
        "id": "ydjHLMUrVETp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-ollama\n"
      ],
      "metadata": {
        "id": "fghQ1eAzQKsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. IMPORTS & CONFIGURATION ---\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import uuid\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import albumentations as A\n",
        "from PIL import Image\n",
        "\n",
        "# TrustLens Core Imports\n",
        "from qdrant_client import QdrantClient, models\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Florence-2 Imports\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "# LlamaIndex Imports\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, Literal\n",
        "from llama_index.core import Settings, Document, VectorStoreIndex\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration Constants\n",
        "PAN_TEMPLATE_PATH = \"/content/convolve_pan_template.jpeg\"\n",
        "DB_TEMPLATES = \"trustlens_templates\"\n",
        "DB_NETWORK = \"trustlens_network\"\n",
        "VECTOR_SIZE = 512\n",
        "DB_PATH = \"/content/qdrant_db\"\n",
        "\n",
        "# --- 2. MODEL INITIALIZATION ---\n",
        "\n",
        "# A. Embedding Model\n",
        "print(\"üß† Loading Embedding Model (BAAI/bge-base-en-v1.5)...\")\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-base-en-v1.5\"\n",
        ")\n",
        "\n",
        "# B. LLM (Switched to Gemma 3 4B as requested)\n",
        "print(\"‚ú® Initializing Gemma 3 4B (Ollama)...\")\n",
        "Settings.llm = Ollama(\n",
        "    model=\"gemma3:4b\",   # <--- Updated to 4B\n",
        "    request_timeout=300.0,\n",
        "    json_mode=True,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Schema (Kept Robust with Optional)\n",
        "class PanCard(BaseModel):\n",
        "    doc_type: Literal[\"pan_card\"] = \"pan_card\"\n",
        "    full_name: Optional[str] = Field(description=\"Full name\")\n",
        "    fathers_name: Optional[str] = Field(description=\"Father's name\")\n",
        "    date_of_birth: Optional[str] = Field(description=\"DOB (DD/MM/YYYY)\")\n",
        "    pan_number: Optional[str] = Field(description=\"PAN Number\")\n",
        "\n",
        "# --- 3. TRUSTLENS PIPELINE CLASS ---\n",
        "if 'GLOBAL_QDRANT_CLIENT' not in globals():\n",
        "    GLOBAL_QDRANT_CLIENT = None\n",
        "\n",
        "class TrustLensPipeline:\n",
        "    def __init__(self):\n",
        "        print(\"üöÄ Initializing TrustLens (Hybrid w/ Florence-2 + Gemma 3 4B)...\")\n",
        "        self.qdrant = self._get_or_create_qdrant_client()\n",
        "\n",
        "        # A. VISUAL AGENT (CLIP)\n",
        "        print(\"üß† Loading CLIP (Visual Semantics)...\")\n",
        "        self.clip = SentenceTransformer('clip-ViT-B-32')\n",
        "        self.doc_labels = [\"Indian PAN Card\", \"Aadhaar Card\", \"Driving License\", \"Random Object\"]\n",
        "        self.label_embeddings = self.clip.encode(self.doc_labels, convert_to_tensor=True)\n",
        "\n",
        "        # B. OCR AGENT (Florence-2)\n",
        "        print(\"üëÅÔ∏è Loading Florence-2 (Vision-Language Model)...\")\n",
        "        self.ocr_model_id = \"microsoft/Florence-2-large\"\n",
        "\n",
        "        # Load Config & Patch\n",
        "        config = AutoConfig.from_pretrained(self.ocr_model_id, trust_remote_code=True)\n",
        "        if not hasattr(config, 'forced_bos_token_id'):\n",
        "            setattr(config, 'forced_bos_token_id', 1)\n",
        "        if not hasattr(config.__class__, 'forced_bos_token_id'):\n",
        "            setattr(config.__class__, 'forced_bos_token_id', 1)\n",
        "\n",
        "        # Load Model\n",
        "        self.ocr_model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.ocr_model_id,\n",
        "            config=config,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16\n",
        "        ).eval().cuda()\n",
        "\n",
        "        self.ocr_processor = AutoProcessor.from_pretrained(\n",
        "            self.ocr_model_id,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # C. BIOMETRIC AGENT\n",
        "        print(\"üë§ Initializing FaceNet...\")\n",
        "        try:\n",
        "            DeepFace.build_model(\"Facenet512\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # D. AUGMENTATION\n",
        "        self.aug = A.Compose([\n",
        "            A.Rotate(limit=10, p=0.8),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.GaussNoise(var_limit=(10.0, 30.0), p=0.4),\n",
        "            A.Perspective(scale=(0.02, 0.05), p=0.4)\n",
        "        ])\n",
        "\n",
        "        self._setup_databases()\n",
        "        print(\"‚úÖ System Ready.\")\n",
        "\n",
        "    def _get_or_create_qdrant_client(self):\n",
        "        global GLOBAL_QDRANT_CLIENT\n",
        "        if GLOBAL_QDRANT_CLIENT is not None: return GLOBAL_QDRANT_CLIENT\n",
        "        client = QdrantClient(path=DB_PATH)\n",
        "        GLOBAL_QDRANT_CLIENT = client\n",
        "        return client\n",
        "\n",
        "    def _setup_databases(self):\n",
        "        self.qdrant.recreate_collection(\n",
        "            collection_name=DB_TEMPLATES,\n",
        "            vectors_config=models.VectorParams(size=VECTOR_SIZE, distance=models.Distance.COSINE)\n",
        "        )\n",
        "        if not self.qdrant.collection_exists(DB_NETWORK):\n",
        "            self.qdrant.recreate_collection(\n",
        "                collection_name=DB_NETWORK,\n",
        "                vectors_config=models.VectorParams(size=VECTOR_SIZE, distance=models.Distance.COSINE)\n",
        "            )\n",
        "            self.qdrant.create_payload_index(DB_NETWORK, \"pan_number\", models.PayloadSchemaType.KEYWORD)\n",
        "\n",
        "    def get_embedding(self, image_source):\n",
        "        if isinstance(image_source, str): img = Image.open(image_source)\n",
        "        else: img = image_source\n",
        "        return self.clip.encode(img).tolist()\n",
        "\n",
        "    # --- STEP 1: READ TEXT (Florence-2) ---\n",
        "    def read_text_with_florence(self, image_path):\n",
        "        \"\"\"Uses Florence-2 to extract all text from the image.\"\"\"\n",
        "        print(\"   üëÅÔ∏è Florence-2: Reading Text...\")\n",
        "\n",
        "        image = Image.open(image_path)\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        task_prompt = \"<OCR>\"\n",
        "\n",
        "        inputs = self.ocr_processor(text=task_prompt, images=image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "\n",
        "        generated_ids = self.ocr_model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=1024,\n",
        "            do_sample=False,\n",
        "            num_beams=3,\n",
        "        )\n",
        "\n",
        "        generated_text = self.ocr_processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "        parsed_answer = self.ocr_processor.post_process_generation(\n",
        "            generated_text,\n",
        "            task=task_prompt,\n",
        "            image_size=(image.width, image.height)\n",
        "        )\n",
        "\n",
        "        return parsed_answer['<OCR>']\n",
        "\n",
        "    # --- STEP 2: STRUCTURE METADATA (GEMMA 3) ---\n",
        "    def structure_metadata_with_gemma(self, raw_text):\n",
        "        \"\"\"Uses Gemma 3 to parse the raw text into JSON.\"\"\"\n",
        "        print(\"   ‚ú® Gemma 3: Structuring Metadata...\")\n",
        "\n",
        "        try:\n",
        "            doc = Document(text=raw_text)\n",
        "            index = VectorStoreIndex.from_documents([doc])\n",
        "            query_engine = index.as_query_engine()\n",
        "\n",
        "            prompt = (\n",
        "                f\"Raw Text: \\\"{raw_text}\\\"\\n\\n\"\n",
        "                \"You are an expert ID card data extractor. Extract the details from the Raw Text above into a JSON object.\\n\"\n",
        "                \"Strictly follow this JSON schema:\\n\"\n",
        "                \"{\\n\"\n",
        "                \"  \\\"full_name\\\": \\\"string or null\\\",\\n\"\n",
        "                \"  \\\"fathers_name\\\": \\\"string or null\\\",\\n\"\n",
        "                \"  \\\"date_of_birth\\\": \\\"DD/MM/YYYY or null\\\",\\n\"\n",
        "                \"  \\\"pan_number\\\": \\\"10 character string or null\\\"\\n\"\n",
        "                \"}\\n\"\n",
        "                \"Output ONLY the valid JSON object. Do not add markdown formatting or explanations.\"\n",
        "            )\n",
        "\n",
        "            response = query_engine.query(prompt)\n",
        "            response_text = str(response).strip()\n",
        "\n",
        "            match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "            if match:\n",
        "                clean_json = match.group(0)\n",
        "            else:\n",
        "                clean_json = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "            data_dict = json.loads(clean_json)\n",
        "\n",
        "            # Robustness check\n",
        "            if \"pan_number\" not in data_dict:\n",
        "                print(f\"   ‚ö†Ô∏è Metadata warning: 'pan_number' missing in JSON.\")\n",
        "\n",
        "            return PanCard(**data_dict).model_dump()\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"   ‚ö†Ô∏è JSON Parse Error. Raw Gemma Output: {response_text}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Gemma Parsing Failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def search_by_pan(self, pan_number):\n",
        "        hits = self.qdrant.scroll(\n",
        "            collection_name=DB_NETWORK,\n",
        "            scroll_filter=models.Filter(must=[models.FieldCondition(key=\"pan_number\", match=models.MatchValue(value=pan_number))]),\n",
        "            limit=1\n",
        "        )[0]\n",
        "        return {\"status\": \"FOUND\", \"data\": hits[0].payload} if hits else {\"status\": \"NOT FOUND\"}\n",
        "\n",
        "    # --- MAIN PIPELINE ---\n",
        "    def verify_document(self, input_path):\n",
        "        print(f\"\\nüîç Processing: {input_path}\")\n",
        "\n",
        "        # 1. Semantic Check (CLIP)\n",
        "        img_emb = self.clip.encode(Image.open(input_path), convert_to_tensor=True)\n",
        "        scores = util.cos_sim(img_emb, self.label_embeddings)[0]\n",
        "        if self.doc_labels[np.argmax(scores.cpu().numpy())] != \"Indian PAN Card\":\n",
        "            return \"‚ùå REJECTED: Wrong Document Type\"\n",
        "\n",
        "        # 2. Structure Check (Vector Search - BGE Base)\n",
        "        vector = self.get_embedding(input_path)\n",
        "        layout_hits = self.qdrant.query_points(collection_name=DB_TEMPLATES, query=vector, limit=1).points\n",
        "        if not layout_hits or layout_hits[0].score < 0.78:\n",
        "            return \"‚ùå REJECTED: Structural Mismatch\"\n",
        "\n",
        "        # 3. Fraud Check (Duplicate Image)\n",
        "        fraud_hits = self.qdrant.query_points(collection_name=DB_NETWORK, query=vector, limit=1).points\n",
        "        if fraud_hits and fraud_hits[0].score > 0.98:\n",
        "            return \"‚ùå REJECTED: FRAUD (Duplicate Image Submission)\"\n",
        "\n",
        "        # 4. Biometrics\n",
        "        try:\n",
        "            DeepFace.extract_faces(img_path=input_path, detector_backend=\"opencv\", enforce_detection=True)\n",
        "        except:\n",
        "            return \"‚ùå REJECTED: No Face Detected.\"\n",
        "\n",
        "        # 5. DATA EXTRACTION (Hybrid: Florence-2 + Gemma 3)\n",
        "        raw_text = self.read_text_with_florence(input_path)\n",
        "        metadata = self.structure_metadata_with_gemma(raw_text)\n",
        "\n",
        "        if not metadata:\n",
        "             return \"‚ö†Ô∏è REJECTED: Metadata Extraction Failed.\"\n",
        "\n",
        "        # --- [NEW] DISPLAY FULL METADATA ---\n",
        "        print(\"\\nüìÑ FULL EXTRACTED METADATA:\")\n",
        "        print(json.dumps(metadata, indent=4))\n",
        "        print(\"-\" * 30)\n",
        "        # -----------------------------------\n",
        "\n",
        "        pan_id = metadata.get(\"pan_number\")\n",
        "        full_name = metadata.get(\"full_name\")\n",
        "\n",
        "        # 6. Database Check (Identity Theft)\n",
        "        if pan_id:\n",
        "            db_check = self.search_by_pan(pan_id)\n",
        "            if db_check['status'] == \"FOUND\":\n",
        "                 return f\"‚ùå REJECTED: FRAUD (PAN {pan_id} already registered)\"\n",
        "\n",
        "        # 7. Success -> Index\n",
        "        self.qdrant.upsert(\n",
        "            collection_name=DB_NETWORK,\n",
        "            points=[models.PointStruct(\n",
        "                id=str(uuid.uuid4()),\n",
        "                vector=vector,\n",
        "                payload={\n",
        "                    \"source\": input_path,\n",
        "                    \"status\": \"verified\",\n",
        "                    \"pan_number\": pan_id,\n",
        "                    \"metadata\": metadata\n",
        "                }\n",
        "            )]\n",
        "        )\n",
        "        return f\"‚úÖ ACCEPTED & INDEXED: {pan_id}\"\n",
        "\n",
        "    def index_template(self, template_path):\n",
        "        print(f\"üìÇ Indexing Template: {template_path}\")\n",
        "        original_vec = self.get_embedding(template_path)\n",
        "        points = [models.PointStruct(id=0, vector=original_vec, payload={\"type\": \"original\"})]\n",
        "        img_cv = cv2.imread(template_path)\n",
        "        img_cv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "        for i in range(5):\n",
        "            aug_data = self.aug(image=img_cv)[\"image\"]\n",
        "            aug_pil = Image.fromarray(aug_data)\n",
        "            aug_vec = self.clip.encode(aug_pil).tolist()\n",
        "            points.append(models.PointStruct(id=i+1, vector=aug_vec, payload={\"type\": \"augmented\"}))\n",
        "        self.qdrant.upsert(collection_name=DB_TEMPLATES, points=points)\n",
        "\n"
      ],
      "metadata": {
        "id": "QG1Ip0K3i2Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INITIALIZATION ---\n",
        "PAN_TEMPLATE_PATH = \"/content/convolve_pan_template.jpeg\"\n",
        "agent = TrustLensPipeline()\n",
        "if os.path.exists(PAN_TEMPLATE_PATH):\n",
        "    agent.index_template(PAN_TEMPLATE_PATH)"
      ],
      "metadata": {
        "id": "m3UU4XTvzj10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# üì∏ PASTE YOUR UPLOADED IMAGE NAME HERE\n",
        "# ----------------------------------------\n",
        "TEST_IMAGE = \"/content/Pan_Test1.jpeg\"\n",
        "\n",
        "if os.path.exists(TEST_IMAGE):\n",
        "    result = agent.verify_document(TEST_IMAGE)\n",
        "    print(f\"\\nüèÜ FINAL RESULT: {result}\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: File '{TEST_IMAGE}' not found.\")"
      ],
      "metadata": {
        "id": "mq4oPOHzjaGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6_POLf133hOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# üîé SEARCH DATABASE & DISPLAY IMAGE (Safe Version)\n",
        "# ----------------------------------------\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as IPImage  # <--- ALIASING PREVENTS THE BUG\n",
        "import os\n",
        "\n",
        "# 1. Input the PAN Number to fetch\n",
        "QUERY_PAN = \"BCFGH123AD\"  # <--- REPLACE WITH TARGET PAN\n",
        "\n",
        "print(f\"üîé Searching Database for PAN: {QUERY_PAN}...\")\n",
        "\n",
        "# 2. Search the Qdrant DB\n",
        "result = agent.search_by_pan(QUERY_PAN)\n",
        "\n",
        "# 3. Process & Display\n",
        "if result[\"status\"] == \"FOUND\":\n",
        "    # Extract the actual data payload\n",
        "    record = result[\"data\"]\n",
        "\n",
        "    # Get fields\n",
        "    stored_pan = record.get(\"pan_number\")\n",
        "    image_path = record.get(\"source\")\n",
        "    metadata   = record.get(\"metadata\", {})\n",
        "    status     = record.get(\"status\")\n",
        "\n",
        "    print(\"\\n‚úÖ RECORD FOUND:\")\n",
        "    print(f\"   üÜî PAN ID:     {stored_pan}\")\n",
        "    print(f\"   üë§ Name:       {metadata.get('full_name', 'N/A')}\")\n",
        "    print(f\"   üõ°Ô∏è Status:     {status}\")\n",
        "    print(f\"   üìÇ Local Path: {image_path}\")\n",
        "\n",
        "    # 4. Fetch and Display Image\n",
        "    if image_path and os.path.exists(image_path):\n",
        "        print(\"\\nüëá RETRIEVED DOCUMENT IMAGE:\")\n",
        "        # We use IPImage here so we don't break PIL's 'Image'\n",
        "        display(IPImage(filename=image_path, width=400))\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"‚ùå CRITICAL ERROR: Image file for PAN {QUERY_PAN} is missing at path: '{image_path}'\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"üö´ NOT FOUND: PAN '{QUERY_PAN}' does not exist in the registry.\")"
      ],
      "metadata": {
        "id": "_jshww4eCW3B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
